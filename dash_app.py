# dash_app.py - Application Streamlit corrig√©e

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from sklearn.cluster import DBSCAN
from sklearn.ensemble import IsolationForest
import joblib
import os
from datetime import datetime, timedelta
from scipy.spatial import ConvexHull
from geopy.distance import geodesic
import time
import threading
import math
import json  # Ajout de l'import json
from sqlalchemy import create_engine, Column, Integer, Float, DateTime, func
from sqlalchemy.orm import declarative_base
from sqlalchemy.orm import sessionmaker
from openai import OpenAI
# Supprimez la duplication
import folium
from streamlit_folium import folium_static, st_folium
from folium.plugins import HeatMap, MarkerCluster
from flask import Flask, request, jsonify
import threading
from flask_cors import CORS

# Dans la section o√π vous cr√©ez l'application Flask
flask_app = Flask(__name__)
CORS(flask_app, resources={r"/api/*": {"origins": "*"}}) 


@flask_app.route("/api/push_data", methods=["POST"])
def api_push_data():
    print("Requ√™te re√ßue:", request.headers)
    print("Donn√©es:", request.get_data())
    if not request.json:
        return jsonify({"error": "No JSON body"}), 400

    body = request.json

    # Si les donn√©es sont envoy√©es sous la cl√© "data", d√©coder la cha√Æne JSON imbriqu√©e
    if "data" in body:
        try:
            if isinstance(body["data"], str):
                body = json.loads(body["data"])
            else:
                body = body["data"]
        except json.JSONDecodeError as e:
            return jsonify({"error": "Invalid JSON format", "details": str(e)}), 400

    try:
        lat = float(body.get("latitude", 0))
        lon = float(body.get("longitude", 0))
        speed = float(body.get("speed", 0))
    except ValueError:
        return jsonify({"error": "Invalid numeric values"}), 400

    # Ajouter la donn√©e avec votre fonction push_data existante
    result = push_data(lat, lon, speed)
    
    return jsonify(result), 200

# Fonction pour d√©marrer le serveur Flask dans un thread s√©par√©
def run_flask_server():
    # Configurer le port (diff√©rent de celui de Streamlit)
    port = int(os.environ.get("FLASK_PORT", 5001))
    flask_app.run(host="0.0.0.0", port=port, debug=False, use_reloader=False)
# Configuration de la page
st.set_page_config(
    page_title="Analyse de Donn√©es Spatiales",
    page_icon="üó∫Ô∏è",
    layout="wide"
)

# ---------------------------------------------------
# 1. Configuration de la base de donn√©es
# ---------------------------------------------------
# Vous pouvez int√©grer directement votre base de donn√©es dans Streamlit
@st.cache_resource

def init_db():
    # Utiliser l'URL externe de la base de donn√©es PostgreSQL de Render
    db_url = "postgresql://data_suivi_user:oYFlVBF6UAaRZk3el2vtXhVPtvOn9uzW@dpg-cuue8c52ng1s739p7grg-a.oregon-postgres.render.com/data_suivi"
    
    engine = create_engine(db_url)
    Base = declarative_base()
    
    # D√©finir le mod√®le SensorData
    class SensorData(Base):
        __tablename__ = 'sensor_data'
        id = Column(Integer, primary_key=True)
        latitude = Column(Float, nullable=False)
        longitude = Column(Float, nullable=False)
        speed = Column(Float, nullable=False)
        # Pas de colonne timestamp pour √©viter l'erreur pr√©c√©dente
    
    # Cr√©er les tables si elles n'existent pas
    Base.metadata.create_all(engine)
    
    # Cr√©er une session
    Session = sessionmaker(bind=engine)
    
    return {"engine": engine, "Session": Session, "SensorData": SensorData, "func": func}

# Initialiser la base de donn√©es
db = init_db()
SensorData = db.get("SensorData")
Session = db.get("Session")

# ---------------------------------------------------
# 2. Variables globales pour l'analyse
# ---------------------------------------------------
if 'last_analysis_time' not in st.session_state:
    st.session_state.last_analysis_time = 0

if 'current_explanation' not in st.session_state:
    st.session_state.current_explanation = "Pas d'analyse disponible."

if 'analysis_in_progress' not in st.session_state:
    st.session_state.analysis_in_progress = False

if 'ml_models' not in st.session_state:
    st.session_state.ml_models = {
        'isolation_forest': None,
        'dbscan': None,
        'is_trained': False
    }

# Initialiser la session state pour les points s√©lectionn√©s
if 'selected_points' not in st.session_state:
    st.session_state.selected_points = []

# ---------------------------------------------------
# 3. Fonctions utilitaires
# ---------------------------------------------------


# Fonction pour calculer la distance entre deux points en m√®tres
def haversine_distance(lat1, lon1, lat2, lon2):
    """Calcule la distance entre deux points en m√®tres"""
    # Convertir en radians
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    
    # Formule de haversine
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
    c = 2 * np.arcsin(np.sqrt(a))
    km = 6371 * c  # Rayon de la Terre en km
    return km * 1000  # En m√®tres

# Fonction pour calculer la distance totale du parcours
def calculate_distance(df):
    """Calcule la distance totale parcourue en km"""
    if len(df) < 2:
        return 0
    
    total_distance = 0
    for i in range(1, len(df)):
        # Calcul de la distance entre deux points cons√©cutifs
        lat1, lon1 = df.iloc[i-1]['latitude'], df.iloc[i-1]['longitude']
        lat2, lon2 = df.iloc[i]['latitude'], df.iloc[i]['longitude']
        
        # Conversion en radians
        lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
        
        # Formule de haversine
        dlon = lon2 - lon1
        dlat = lat2 - lat1
        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
        c = 2 * np.arcsin(np.sqrt(a))
        distance = 6371 * c  # Rayon de la Terre en km
        
        total_distance += distance
    
    return total_distance

# Fonction pour calculer la dur√©e
def calculate_duration(df):
    """Calcule la dur√©e totale entre le premier et le dernier point"""
    if len(df) < 2 or 'timestamp' not in df.columns:
        return "Non disponible"
    
    # S'assurer que timestamp est bien un datetime
    if df['timestamp'].dtype == 'object':
        df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # Calculer la diff√©rence
    start_time = df['timestamp'].min()
    end_time = df['timestamp'].max()
    duration = end_time - start_time
    
    # Formater la dur√©e en heures:minutes:secondes
    hours, remainder = divmod(duration.total_seconds(), 3600)
    minutes, seconds = divmod(remainder, 60)
    
    return f"{int(hours)}h {int(minutes)}m {int(seconds)}s"

# Fonction pour d√©terminer si un point est sur un segment
def is_point_on_segment(lat_p, lon_p, lat_a, lon_a, lat_b, lon_b, width=30):
    """
    D√©termine si un point P(lat_p, lon_p) est sur un segment AB, √† une distance maximale de 'width' m√®tres.
    
    Args:
        lat_p, lon_p: Coordonn√©es du point √† tester
        lat_a, lon_a: Coordonn√©es du point A du segment
        lat_b, lon_b: Coordonn√©es du point B du segment
        width: Distance maximale en m√®tres du point au segment
        
    Returns:
        bool: True si le point est sur le segment √† la distance sp√©cifi√©e, False sinon
    """
    # Calculer la distance du point √† la ligne
    def distance_point_to_line(p, a, b):
        # Convertir en coordonn√©es cart√©siennes simplifi√©es
        R = 111320.0  # M√®tres par degr√© approximativement
        ax = a[1] * R * math.cos(math.radians(a[0]))
        ay = a[0] * R
        bx = b[1] * R * math.cos(math.radians(b[0]))
        by = b[0] * R
        px = p[1] * R * math.cos(math.radians(p[0]))
        py = p[0] * R
        
        # Vecteur AB
        abx = bx - ax
        aby = by - ay
        
        # Vecteur AP
        apx = px - ax
        apy = py - ay
        
        # Projection de AP sur AB
        ab_length_squared = abx**2 + aby**2
        
        if ab_length_squared == 0:  # A = B
            return math.sqrt((px-ax)**2 + (py-ay)**2)
        
        t = max(0, min(1, (apx*abx + apy*aby) / ab_length_squared))
        
        # Point de projection
        projx = ax + t * abx
        projy = ay + t * aby
        
        # Distance du point √† la projection
        return math.sqrt((px-projx)**2 + (py-projy)**2)
    
    dist = distance_point_to_line([lat_p, lon_p], [lat_a, lon_a], [lat_b, lon_b])
    return dist <= width

# ---------------------------------------------------
# 4. Fonctions d'analyse ML
# ---------------------------------------------------
def train_ml_models():
    """Entra√Æne les mod√®les ML avec les donn√©es existantes"""
    try:
        # R√©cup√©rer toutes les donn√©es
        session = Session()
        points = session.query(SensorData).all()
        
        if len(points) < 10:
            st.warning("‚ö†Ô∏è Pas assez de donn√©es pour l'entra√Ænement (minimum 10 points)")
            return False
            
        # Pr√©parer les donn√©es pour le clustering g√©ographique
        coords = np.array([[p.latitude, p.longitude] for p in points])
        speeds = np.array([[p.speed] for p in points])
        
        # DBSCAN pour le clustering des zones
        eps_meters = 50  # 50 m√®tres entre points pour former un cluster
        kms_per_radian = 6371.0088  # Rayon de la Terre en km
        epsilon = eps_meters / 1000 / kms_per_radian
        
        # Entra√Æner DBSCAN
        dbscan = DBSCAN(
            eps=epsilon, 
            min_samples=5, 
            algorithm='ball_tree',
            metric='haversine'
        )
        dbscan.fit(np.radians(coords))
        
        # Isolation Forest pour d√©tecter les anomalies de vitesse
        iso_forest = IsolationForest(contamination=0.05, random_state=42)
        iso_forest.fit(speeds)
        
        # Sauvegarder les mod√®les dans la session state
        st.session_state.ml_models['isolation_forest'] = iso_forest
        st.session_state.ml_models['dbscan'] = dbscan
        st.session_state.ml_models['is_trained'] = True
        
        # Sauvegarder sur disque (optionnel)
        model_dir = "ml_models"
        os.makedirs(model_dir, exist_ok=True)
        
        joblib.dump(iso_forest, os.path.join(model_dir, "iso_forest.pkl"))
        joblib.dump(dbscan, os.path.join(model_dir, "dbscan.pkl"))
        
        st.success(f"‚úÖ Mod√®les ML entra√Æn√©s avec succ√®s sur {len(points)} points")
        return True
        
    except Exception as e:
        st.error(f"‚ùå Erreur lors de l'entra√Ænement ML: {e}")
        return False
    finally:
        session.close()
def load_ml_models():
    """Tente de charger les mod√®les pr√©existants"""
    try:
        model_dir = "ml_models"
        iso_path = os.path.join(model_dir, "iso_forest.pkl")
        dbscan_path = os.path.join(model_dir, "dbscan.pkl")
        
        if os.path.exists(iso_path) and os.path.exists(dbscan_path):
            st.session_state.ml_models['isolation_forest'] = joblib.load(iso_path)
            st.session_state.ml_models['dbscan'] = joblib.load(dbscan_path)
            st.session_state.ml_models['is_trained'] = True
            return True
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Impossible de charger les mod√®les ML: {e}")
    
    return False
def analyze_point(lat, lon, speed):
    """Analyse un point avec les mod√®les ML"""
    if not st.session_state.ml_models['is_trained']:
        if not load_ml_models():
            # Tenter un entra√Ænement si pas de mod√®les pr√©existants
            train_ml_models()
        
        if not st.session_state.ml_models['is_trained']:
            return {
                "status": "no_model",
                "message": "Aucun mod√®le ML disponible"
            }
    
    result = {"status": "success"}
    
    # Analyser la vitesse avec Isolation Forest
    if st.session_state.ml_models['isolation_forest'] is not None:
        # Pr√©diction d'anomalie
        speed_array = np.array([[speed]])
        is_anomaly = st.session_state.ml_models['isolation_forest'].predict(speed_array)[0] == -1
        anomaly_score = st.session_state.ml_models['isolation_forest'].score_samples(speed_array)[0]
        
        result["speed_analysis"] = {
            "is_anomaly": bool(is_anomaly),
            "anomaly_score": float(anomaly_score),
            "message": "Vitesse anormale d√©tect√©e" if is_anomaly else "Vitesse normale"
        }
    
    # Trouver les zones populaires
    session = Session()
    points = session.query(SensorData).all()
    if len(points) >= 10:
        clusters = {}
        coords = np.array([[p.latitude, p.longitude] for p in points])
        
        if st.session_state.ml_models['dbscan'] is not None:
            # Appliquer les labels de cluster √† tous les points
            labels = st.session_state.ml_models['dbscan'].labels_
            
            # Compter les points par cluster et trouver les centres
            for i, label in enumerate(labels):
                if label != -1:  # Ignorer les points consid√©r√©s comme du bruit
                    if label not in clusters:
                        clusters[label] = {
                            "count": 0,
                            "lat_sum": 0,
                            "lon_sum": 0,
                            "points": []
                        }
                    clusters[label]["count"] += 1
                    clusters[label]["lat_sum"] += coords[i][0]
                    clusters[label]["lon_sum"] += coords[i][1]
                    clusters[label]["points"].append(coords[i])
            
            # Calculer les centres de cluster
            popular_zones = []
            for label, data in clusters.items():
                if data["count"] >= 5:  # Au moins 5 points
                    center_lat = data["lat_sum"] / data["count"]
                    center_lon = data["lon_sum"] / data["count"]
                    popular_zones.append({
                        "cluster_id": int(label),
                        "center": [float(center_lat), float(center_lon)],
                        "point_count": data["count"]
                    })
            
            # Trier par nombre de points
            popular_zones.sort(key=lambda x: x["point_count"], reverse=True)
            
            # Garder les 3 premiers
            result["popular_zones"] = popular_zones[:3]
            
            # V√©rifier si le point actuel est dans une zone populaire
            is_in_zone = False
            for zone in popular_zones[:3]:
                # Distance approximative
                dist = haversine_distance(lat, lon, zone["center"][0], zone["center"][1])
                if dist < 50:  # 50 m√®tres
                    is_in_zone = True
                    result["current_zone"] = {
                        "in_popular_zone": True,
                        "zone_rank": popular_zones.index(zone) + 1,
                        "cluster_id": zone["cluster_id"],
                        "point_count": zone["point_count"]
                    }
                    break
            
            if not is_in_zone:
                result["current_zone"] = {"in_popular_zone": False}
    
    session.close()
    return result
def analyser_ralentissement_async(speed, avg_speed):
    """
    Fonction asynchrone qui analyse le ralentissement avec OpenAI
    """
    try:
        st.session_state.analysis_in_progress = True
        
        # V√©rifier si une cl√© API OpenAI est disponible
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            st.session_state.current_explanation = "Cl√© API OpenAI non disponible"
            return
        
        # Initialiser le client OpenAI
        client = OpenAI(api_key=api_key)
        
        # Appel API avec un timeout court et un prompt simplifi√©
        response = client.chat.completions.create(
            model="gpt-4",  
            messages=[
                {"role": "system", "content": "Tu es un expert trafic. R√©ponse courte (30 mots max)."},
                {"role": "user", "content": f"Pourquoi vitesse={speed}m/s vs moyenne={avg_speed:.2f}m/s? Raison courte."}
            ],
            max_tokens=50
        )
        
        # Extraction de la r√©ponse
        if hasattr(response, 'choices') and response.choices:
            result = response.choices[0].message.content
            st.session_state.current_explanation = result
        else:
            st.session_state.current_explanation = "Aucune explication trouv√©e."
            
    except Exception as e:
        st.session_state.current_explanation = f"Erreur d'analyse: {str(e)}"
    finally:
        st.session_state.analysis_in_progress = False
def display_anomaly_results(df_anomalies):
                # Cr√©ation de la carte avec anomalies
                m_anomalies = folium.Map(
                    location=[df_anomalies['latitude'].mean(), df_anomalies['longitude'].mean()], 
                    zoom_start=14
                )
                
                # Taille des points
                point_size = 5
                
                # Tracer les points normaux
                for idx, row in df_anomalies[df_anomalies['anomaly'] == 1].iterrows():
                    folium.CircleMarker(
                        [row['latitude'], row['longitude']],
                        radius=point_size,
                        color='blue',
                        fill=True,
                        fill_opacity=0.5,
                        popup=f"Normal: {row['speed']:.2f} m/s"
                    ).add_to(m_anomalies)
                
                # Tracer les anomalies avec taille proportionnelle √† l'ampleur de l'anomalie
                for idx, row in df_anomalies[df_anomalies['anomaly'] == -1].iterrows():
                    # Calculer la taille en fonction du score d'anomalie
                    # Plus le score est bas, plus l'anomalie est forte
                    score = row['anomaly_score']
                    # Normalisation: plus le score est n√©gatif, plus l'anomalie est importante
                    normalized_score = abs(min(score, 0))
                    # Taille variable: entre taille de base et taille de base + 5
                    radius = point_size + (normalized_score * 5)
                    
                    # D√©terminer la couleur en fonction de la vitesse
                    if row['speed'] <= 0.5:  # Vitesse tr√®s basse
                        color = 'purple'
                        reason = "Arr√™t ou vitesse tr√®s basse"
                    elif row['speed'] >= 15:  # Vitesse tr√®s √©lev√©e
                        color = 'red'
                        reason = "Vitesse tr√®s √©lev√©e"
                    else:  # Autres anomalies
                        color = 'orange'
                        reason = "Anomalie de comportement"
                    
                    folium.CircleMarker(
                        [row['latitude'], row['longitude']],
                        radius=radius,
                        color=color,
                        fill=True,
                        fill_opacity=0.7,
                        popup=f"Anomalie: {row['speed']:.2f} m/s<br>Score: {row['anomaly_score']:.3f}<br>Raison: {reason}"
                    ).add_to(m_anomalies)
                
                # Afficher la carte avec anomalies
                st.subheader("Carte des anomalies d√©tect√©es")
                folium_static(m_anomalies, width=800, height=500)
                
                # Statistiques des anomalies
                n_anomalies = (df_anomalies['anomaly'] == -1).sum()
                
                col1, col2, col3 = st.columns(3)
                col1.metric("Nombre d'anomalies", n_anomalies)
                col2.metric("Pourcentage d'anomalies", f"{(n_anomalies / len(df_anomalies) * 100):.1f}%")
                
                # Si on a des vitesses tr√®s faibles ou tr√®s √©lev√©es
                n_low_speed = ((df_anomalies['anomaly'] == -1) & (df_anomalies['speed'] <= 0.5)).sum()
                n_high_speed = ((df_anomalies['anomaly'] == -1) & (df_anomalies['speed'] >= 15)).sum()
                
                col3.metric("Anomalies de vitesse", f"{n_low_speed} basses, {n_high_speed} √©lev√©es")
                
                # Distribution des scores d'anomalies
                fig = px.histogram(
                    df_anomalies, 
                    x='anomaly_score', 
                    color='anomaly',
                    color_discrete_map={1: 'blue', -1: 'red'},
                    labels={"anomaly_score": "Score d'anomalie", "count": "Nombre de points"},
                    title="Distribution des scores d'anomalie"
                )
                st.plotly_chart(fig, use_container_width=True)
                
                # Tableau des anomalies d√©tect√©es
                if n_anomalies > 0:
                    st.subheader("Liste des anomalies d√©tect√©es")
                    
                    # Ajouter une description de l'anomalie
                    anomalies_df = df_anomalies[df_anomalies['anomaly'] == -1].copy()
                    
                    # Classer les anomalies
                    def classify_anomaly(row):
                        if row['speed'] <= 0.5:
                            return "Arr√™t ou vitesse tr√®s basse"
                        elif row['speed'] >= 15:
                            return "Vitesse tr√®s √©lev√©e"
                        else:
                            return "Comportement atypique"
                    
                    anomalies_df['type_anomalie'] = anomalies_df.apply(classify_anomaly, axis=1)
                    
                    # Afficher le tableau des anomalies
                    anomalies_display = anomalies_df[['latitude', 'longitude', 'speed', 'anomaly_score', 'type_anomalie']]
                    anomalies_display = anomalies_display.sort_values(by='anomaly_score')
                    st.dataframe(anomalies_display)
# ---------------------------------------------------
# 5. Fonctions de gestion des donn√©es
# ---------------------------------------------------
# Fonction pour afficher les r√©sultats des anomalies

def push_data(lat, lon, speed):
    """Ajoute une nouvelle donn√©e de capteur et analyse les ralentissements"""
    try:
        session = Session()
        
        # Calculer la vitesse moyenne actuelle
        avg_speed = session.query(func.avg(SensorData.speed)).scalar() or 0
        
        # Ajouter les donn√©es
        new_data = SensorData(
            latitude=lat,
            longitude=lon,
            speed=speed
        )
        session.add(new_data)
        session.commit()
        
        # D√©tection d'un ralentissement
        ralentissement = False
        if avg_speed > 0 and speed < avg_speed * 0.8:
            ralentissement = True
            
            # V√©rifier si il faut lancer une analyse
            current_time = time.time()
            if not st.session_state.analysis_in_progress and (current_time - st.session_state.last_analysis_time) > 60:
                st.session_state.last_analysis_time = current_time
                
                # Lancer l'analyse dans un thread s√©par√©
                thread = threading.Thread(
                    target=analyser_ralentissement_async, 
                    args=(speed, avg_speed)
                )
                thread.daemon = True
                thread.start()
        
        return {
            "status": "success",
            "current_speed": speed,
            "average_speed": avg_speed,
            "slowdown_detected": ralentissement
        }
        
    except Exception as e:
        return {"status": "error", "message": str(e)}
    finally:
        session.close()

def get_all_points():
    """R√©cup√®re tous les points de donn√©es"""
    try:
        session = Session()
        points = session.query(SensorData).all()
        data = []
        
        for p in points:
            point_data = {
                "id": p.id,
                "latitude": p.latitude,
                "longitude": p.longitude,
                "speed": p.speed
            }
            # Ajouter timestamp seulement s'il existe
            if hasattr(p, 'timestamp') and p.timestamp:
                point_data["timestamp"] = p.timestamp.isoformat()
            
            data.append(point_data)
        
        return {"status": "success", "points": data}
    except Exception as e:
        return {"status": "error", "message": str(e)}
    finally:
        session.close()

def get_latest_point():
    """R√©cup√®re le dernier point enregistr√©"""
    try:
        session = Session()
        latest = session.query(SensorData).order_by(SensorData.id.desc()).first()
        
        if latest:
            # Pr√©parer les donn√©es du point
            point_data = {
                "id": latest.id,
                "latitude": latest.latitude,
                "longitude": latest.longitude,
                "speed": latest.speed,
                "is_latest": True
            }
            
            # Ajouter timestamp seulement s'il existe
            if hasattr(latest, 'timestamp') and latest.timestamp:
                point_data["timestamp"] = latest.timestamp.isoformat()
            
            # Obtenir les r√©sultats ML
            ml_results = analyze_point(latest.latitude, latest.longitude, latest.speed)
            
            return {
                "status": "success",
                "latest_point": point_data,
                "ml_analysis": ml_results
            }
        else:
            return {
                "status": "error",
                "message": "Aucun point trouv√©"
            }
    except Exception as e:
        return {"status": "error", "message": str(e)}
    finally:
        session.close()

# --- Interface utilisateur Streamlit ---

# Titre de l'application
st.title("Tableau de bord d'analyse de donn√©es spatiales")

# Barre lat√©rale pour la navigation
st.sidebar.title("Navigation")
page = st.sidebar.selectbox(
    "Choisir une page",
    ["Carte principale", "Analyse de vitesse", "Clusters", "Anomalies", "Statistiques", "Configuration"]
)
# Charger les donn√©es
@st.cache_data(ttl=10)  # Cache pendant 10 secondes
def load_data():
    data = get_all_points()
    if data["status"] == "success":
        return pd.DataFrame(data["points"])
    else:
        st.error(f"Erreur de chargement des donn√©es: {data.get('message', 'Erreur inconnue')}")
        return pd.DataFrame()

df = load_data()

# Condition pour afficher une aide si aucune donn√©e
if len(df) == 0:
    st.warning("Aucune donn√©e disponible. Utilisez l'onglet Configuration pour ajouter des points ou importer des donn√©es.")
    if page != "Configuration":
        st.info("Allez dans la page Configuration pour commencer.")

# Fonction pour cr√©er une carte avec points
def create_map_with_points(df, center=None, zoom=14):
    if len(df) == 0:
        # Coordonn√©es par d√©faut (Reims)
        center = [49.2484420, 4.0415017]
        m = folium.Map(location=center, zoom_start=zoom)
        return m
    
    if center is None:
        # Utiliser le centre des donn√©es
        center = [df['latitude'].mean(), df['longitude'].mean()]
    
    m = folium.Map(location=center, zoom_start=zoom)
    
    # Ajouter tous les points en gris
    for idx, row in df.iterrows():
        folium.CircleMarker(
            [row['latitude'], row['longitude']],
            radius=4,
            color='gray',
            fill=True,
            fill_opacity=0.7,
            popup=f"Point {idx}: {row['speed']:.2f} m/s"
        ).add_to(m)
    
    # Ajouter le dernier point en bleu et plus gros s'il existe
    if len(df) > 0:
        latest = df.iloc[-1]
        folium.CircleMarker(
            [latest['latitude'], latest['longitude']],
            radius=8,
            color='#4285F4',
            fill=True,
            fill_opacity=0.8,
            weight=2,
            popup=f"Dernier point: {latest['speed']:.2f} m/s"
        ).add_to(m)
    
    return m

# -- PAGES --
# Remplacez toutes les occurrences de st.experimental_rerun() par st.rerun()
# Par exemple, dans le code de la page "Carte principale" :

# Modification pour la section de la carte principale pour:
# 1. Permettre de s√©lectionner les points existants de la trajectoire
# 2. Am√©liorer l'analyse des segments

if page == "Carte principale":
    st.header("Carte interactive")
    
    # Disposition en colonnes
    col1, col2 = st.columns([3, 1])
    
    # Variable pour stocker temporairement le point s√©lectionn√©
    selected_lat, selected_lng = None, None
    selected_existing_point = False
    
    with col1:
        # Cr√©er une carte avec possibilit√© de clic
        center = [df['latitude'].mean(), df['longitude'].mean()] if len(df) > 0 else [49.2484420, 4.0415017]
        
        # Cr√©er une carte Folium
        m = folium.Map(location=center, zoom_start=14)
        
        # Ajouter les points de donn√©es existants avec des popups cliquables
        if len(df) > 0:
            for idx, row in df.iterrows():
                popup_content = f"""
                <strong>Point {idx}</strong><br>
                Vitesse: {row['speed']:.2f} m/s<br>
                <button onclick="
                    const lat = {row['latitude']};
                    const lng = {row['longitude']};
                    document.getElementById('selected-point-info').innerText = 'Point de trajectoire ' + {idx} + ' s√©lectionn√©';
                    // Simuler un clic sur la carte pour envoyer les coordonn√©es
                    var event = new CustomEvent('trajectory_point_selected', {{ 
                        detail: {{ lat: lat, lng: lng, id: {idx} }}
                    }});
                    document.dispatchEvent(event);
                ">S√©lectionner ce point</button>
                """
                
                folium.CircleMarker(
                    [row['latitude'], row['longitude']],
                    radius=4,
                    color='gray',
                    fill=True,
                    fill_opacity=0.7,
                    popup=folium.Popup(popup_content, max_width=200)
                ).add_to(m)
        
        # Ajouter les points d√©j√† s√©lectionn√©s sur la carte avec num√©rotation
        for i, (lat, lon) in enumerate(st.session_state.selected_points):
            folium.Marker(
                [lat, lon], 
                popup=f"Point de segment {i+1}",
                icon=folium.Icon(color="blue", icon="info-sign")
            ).add_to(m)
        
        # Ajouter les segments entre les points cons√©cutifs
        if len(st.session_state.selected_points) > 1:
            for i in range(1, len(st.session_state.selected_points)):
                start_point = st.session_state.selected_points[i-1]
                end_point = st.session_state.selected_points[i]
                
                folium.PolyLine(
                    [start_point, end_point],
                    color="blue",
                    weight=3,
                    opacity=0.7,
                    popup=f"Segment {i}"
                ).add_to(m)
        
        # D√©sactiver le cache pour cette carte
        st.cache_data.clear()
        
        # Ajouter un gestionnaire d'√©v√©nements personnalis√© avec JavaScript
        js = """
        <script>
        // √âcouter l'√©v√©nement personnalis√© de s√©lection de point de trajectoire
        document.addEventListener('trajectory_point_selected', function(e) {
            // Cr√©er un √©l√©ment cach√© pour stocker les coordonn√©es
            var hiddenInput = document.createElement('input');
            hiddenInput.type = 'hidden';
            hiddenInput.id = 'selected-trajectory-point';
            hiddenInput.value = JSON.stringify({
                lat: e.detail.lat, 
                lng: e.detail.lng,
                id: e.detail.id
            });
            document.body.appendChild(hiddenInput);
        });
        </script>
        
        <div id="selected-point-info" style="color: blue;"></div>
        """
        
        # Injecter le JavaScript dans la carte
        m.get_root().html.add_child(folium.Element(js))
        
        # Utiliser st_folium pour afficher la carte et r√©cup√©rer les informations de clic
        map_data = st_folium(
            m, 
            width=800, 
            height=500,
            returned_objects=["last_clicked"],
            use_container_width=True,
            key="map_main"
        )
        
        # V√©rifier si un clic a √©t√© effectu√© sur la carte
        if map_data and 'last_clicked' in map_data and map_data['last_clicked']:
            clicked = map_data['last_clicked']
            selected_lat, selected_lng = clicked['lat'], clicked['lng']
            
            # V√©rifier si le point est proche d'un point de trajectoire existant
            if len(df) > 0:
                for idx, row in df.iterrows():
                    dist = haversine_distance(
                        selected_lat, selected_lng, 
                        row['latitude'], row['longitude']
                    )
                    # Si le clic est √† moins de 10 m√®tres d'un point existant, utiliser ce point
                    if dist < 10:  # 10 m√®tres de tol√©rance
                        selected_lat = row['latitude']
                        selected_lng = row['longitude']
                        selected_existing_point = True
                        st.success(f"‚úÖ Point de trajectoire {idx} s√©lectionn√© (distance: {dist:.1f}m)")
                        break
            
            if not selected_existing_point:
                st.success(f"üìç Nouveau point s√©lectionn√©: Lat {selected_lat:.6f}, Lng {selected_lng:.6f}")
    
    with col2:
        # M√©triques principales
        st.subheader("Informations g√©n√©rales")
        
        if len(df) > 0:
            latest = df.iloc[-1]
            st.metric("Points enregistr√©s", len(df))
            st.metric("Derni√®re vitesse", f"{latest['speed']:.2f} m/s")
            
            # Calcul de la vitesse moyenne
            avg_speed = df['speed'].mean()
            st.metric("Vitesse moyenne", f"{avg_speed:.2f} m/s")
            
            # Afficher l'explication IA si disponible
            if st.session_state.current_explanation != "Pas d'analyse disponible.":
                st.info(f"**Explication IA:** {st.session_state.current_explanation}")
            
            # Ajouter un indicateur de statut ML
            if st.session_state.ml_models['is_trained']:
                st.success("Mod√®les ML: Entra√Æn√©s")
            else:
                st.warning("Mod√®les ML: Non entra√Æn√©s")
        else:
            st.info("Aucune donn√©e disponible")
        
        # Instructions pour l'utilisateur
        st.info("üìå **Instructions**: Cliquez sur la carte ou sur un point de trajectoire existant pour le s√©lectionner, puis ajoutez-le √† votre segment")
        
        # Afficher les coordonn√©es du point s√©lectionn√© et proposer d'ajouter ce point
        if selected_lat is not None and selected_lng is not None:
            # Bouton pour ajouter le point au segment
            if st.button("Ajouter le point s√©lectionn√© au segment"):
                st.session_state.selected_points.append((selected_lat, selected_lng))
                st.success(f"Point {len(st.session_state.selected_points)} ajout√©!")
                st.rerun()
        
        # Afficher les points actuellement s√©lectionn√©s
        if st.session_state.selected_points:
            st.subheader(f"Points du segment ({len(st.session_state.selected_points)})")
            for i, (lat, lon) in enumerate(st.session_state.selected_points):
                st.text(f"Point {i+1}: Lat {lat:.6f}, Lng {lon:.6f}")
            
            # Boutons pour g√©rer les points
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("R√©initialiser tous les points"):
                    st.session_state.selected_points = []
                    st.success("Points r√©initialis√©s")
                    st.rerun()
            
            with col2:
                if len(st.session_state.selected_points) > 0 and st.button("Supprimer le dernier point"):
                    st.session_state.selected_points.pop()
                    st.success("Dernier point supprim√©")
                    st.rerun()
        
        # Ajouter un bouton pour actualiser les donn√©es mais uniquement √† la demande
        if st.button("Actualiser les donn√©es", key="refresh_button"):
            st.cache_data.clear()
            st.rerun()
    
    # Section pour l'analyse de segments
    if len(st.session_state.selected_points) >= 2:
        st.header("Analyse de segments")
        
        # Cr√©er une carte pour visualiser les segments
        m_segments = folium.Map(
            location=[
                sum(p[0] for p in st.session_state.selected_points) / len(st.session_state.selected_points),
                sum(p[1] for p in st.session_state.selected_points) / len(st.session_state.selected_points)
            ],
            zoom_start=14
        )
        
        # Ajouter les points s√©lectionn√©s comme marqueurs
        for i, (lat, lon) in enumerate(st.session_state.selected_points):
            folium.Marker(
                [lat, lon],
                popup=f"Point {i+1}",
                icon=folium.Icon(color="blue", icon="info-sign")
            ).add_to(m_segments)
        
        # Calculer et tracer tous les segments
        segment_stats = []
        
        # Tracer les segments entre points cons√©cutifs
        for i in range(1, len(st.session_state.selected_points)):
            start_point = st.session_state.selected_points[i-1]
            end_point = st.session_state.selected_points[i]
            
            # Tracer la ligne
            folium.PolyLine(
                [start_point, end_point],
                color="blue",
                weight=3,
                opacity=0.7,
                popup=f"Segment {i}: Point {i} - Point {i+1}"
            ).add_to(m_segments)
            
            # Calculer la distance pour ce segment
            lat1, lon1 = start_point
            lat2, lon2 = end_point
            
            # Conversion en radians
            lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
            
            # Formule de haversine
            dlon = lon2 - lon1
            dlat = lat2 - lat1
            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
            c = 2 * np.arcsin(np.sqrt(a))
            distance = 6371 * c  # Rayon de la Terre en km
            
            # Ajouter aux statistiques
            segment_stats.append({
                "segment_id": i,
                "from_point": i,
                "to_point": i+1,
                "distance_km": distance,
                "points_on_segment": 0,
                "avg_speed": 0,
                "min_speed": float('inf'),
                "max_speed": 0,
                "points_data": []
            })
            
            # Calculer les points sur ce segment
            segment_width = 30  # m√®tres
            points_on_segment = []
            
            for idx, row in df.iterrows():
                if is_point_on_segment(
                    row['latitude'], row['longitude'], 
                    start_point[0], start_point[1], 
                    end_point[0], end_point[1], 
                    segment_width
                ):
                    points_on_segment.append(row)
                    # Ajouter le point √† la carte avec une couleur sp√©cifique au segment
                    folium.CircleMarker(
                        [row['latitude'], row['longitude']],
                        radius=3,
                        color=f"hsl({(i*50) % 360}, 70%, 50%)",
                        fill=True,
                        fill_opacity=0.5,
                        popup=f"Segment {i}: {row['speed']:.2f} m/s"
                    ).add_to(m_segments)
            
            # Mettre √† jour les stats de ce segment
            if points_on_segment:
                speeds = [p['speed'] for p in points_on_segment]
                segment_stats[-1]["points_on_segment"] = len(points_on_segment)
                segment_stats[-1]["avg_speed"] = sum(speeds) / len(speeds)
                segment_stats[-1]["min_speed"] = min(speeds)
                segment_stats[-1]["max_speed"] = max(speeds)
                segment_stats[-1]["points_data"] = points_on_segment
        
        # Ajouter √©galement un segment direct du premier au dernier point (si plus de 2 points)
        if len(st.session_state.selected_points) > 2:
            start_point = st.session_state.selected_points[0]
            end_point = st.session_state.selected_points[-1]
            
            # Tracer la ligne en pointill√©s
            folium.PolyLine(
                [start_point, end_point],
                color="red",
                weight=2,
                opacity=0.7,
                dash_array="5, 10",
                popup=f"Segment direct: Point 1 - Point {len(st.session_state.selected_points)}"
            ).add_to(m_segments)
            
            # Calculer la distance pour ce segment direct
            lat1, lon1 = start_point
            lat2, lon2 = end_point
            
            # Conversion en radians
            lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
            
            # Formule de haversine
            dlon = lon2 - lon1
            dlat = lat2 - lat1
            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2
            c = 2 * np.arcsin(np.sqrt(a))
            distance = 6371 * c  # Rayon de la Terre en km
            
            # Calculer les points sur ce segment direct
            segment_width = 30  # m√®tres
            points_on_direct = []
            
            for idx, row in df.iterrows():
                if is_point_on_segment(
                    row['latitude'], row['longitude'], 
                    start_point[0], start_point[1], 
                    end_point[0], end_point[1], 
                    segment_width
                ):
                    points_on_direct.append(row)
            
            # Pr√©parer les stats pour le segment direct
            direct_stats = {
                "segment_id": "direct",
                "from_point": 1,
                "to_point": len(st.session_state.selected_points),
                "distance_km": distance,
                "points_on_segment": len(points_on_direct),
                "avg_speed": 0,
                "min_speed": float('inf'),
                "max_speed": 0
            }
            
            # Mettre √† jour les stats du segment direct
            if points_on_direct:
                speeds = [p['speed'] for p in points_on_direct]
                direct_stats["avg_speed"] = sum(speeds) / len(speeds)
                direct_stats["min_speed"] = min(speeds)
                direct_stats["max_speed"] = max(speeds)
            
            # Ajouter aux statistiques
            segment_stats.append(direct_stats)
        
        # Afficher la carte
        folium_static(m_segments, width=800, height=500)
        
        # Afficher les statistiques des segments
        st.subheader("Statistiques des segments")
        
        # Cr√©er un DataFrame des statistiques
        stats_columns = ["segment_id", "from_point", "to_point", "distance_km", 
                        "points_on_segment", "avg_speed", "min_speed", "max_speed"]
        stats_df = pd.DataFrame(segment_stats)[stats_columns]
        
        # Formater les chiffres
        for col in ["distance_km", "avg_speed", "min_speed", "max_speed"]:
            if col in stats_df.columns:
                stats_df[col] = stats_df[col].apply(lambda x: round(x, 2))
        
        # Calculer la distance totale des segments cons√©cutifs
        total_distance = sum(stat["distance_km"] for stat in segment_stats if stat["segment_id"] != "direct")
        
        # Afficher les statistiques
        col1, col2, col3 = st.columns(3)
        col1.metric("Distance totale du parcours", f"{total_distance:.2f} km")
        
        if any(stat["points_on_segment"] > 0 for stat in segment_stats if stat["segment_id"] != "direct"):
            avg_speed_all_segments = np.mean([
                stat["avg_speed"] for stat in segment_stats 
                if stat["segment_id"] != "direct" and stat["points_on_segment"] > 0
            ])
            col2.metric("Vitesse moyenne", f"{avg_speed_all_segments:.2f} m/s")
        
        if len(segment_stats) > 0 and "direct" in [str(s["segment_id"]) for s in segment_stats]:
            direct_distance = next(s["distance_km"] for s in segment_stats if s["segment_id"] == "direct")
            col3.metric("Distance directe (vol d'oiseau)", f"{direct_distance:.2f} km")
        
        # Afficher un tableau des statistiques par segment
        st.dataframe(stats_df)
        
        # Graphique comparatif des segments
        st.subheader("Comparaison des segments")
        
        # Filtrer les segments qui ont des points
        segments_with_data = [s for s in segment_stats if s["points_on_segment"] > 0]
        
        if segments_with_data:
            # Comparer les vitesses moyennes des segments
            fig_speed = px.bar(
                x=[f"Segment {s['segment_id']}" if s['segment_id'] != "direct" else "Direct" for s in segments_with_data],
                y=[s["avg_speed"] for s in segments_with_data],
                labels={"x": "Segment", "y": "Vitesse moyenne (m/s)"},
                title="Comparaison des vitesses moyennes par segment",
                color=[
                    "Trajet" if s['segment_id'] != "direct" else "Direct" 
                    for s in segments_with_data
                ],
                color_discrete_map={"Trajet": "blue", "Direct": "red"}
            )
            
            st.plotly_chart(fig_speed, use_container_width=True)
            
            # Comparer la densit√© de points des segments
            fig_density = px.bar(
                x=[f"Segment {s['segment_id']}" if s['segment_id'] != "direct" else "Direct" for s in segments_with_data],
                y=[s["points_on_segment"] / s["distance_km"] for s in segments_with_data],
                labels={"x": "Segment", "y": "Densit√© (points/km)"},
                title="Densit√© de points par segment",
                color=[
                    "Trajet" if s['segment_id'] != "direct" else "Direct" 
                    for s in segments_with_data
                ],
                color_discrete_map={"Trajet": "blue", "Direct": "red"}
            )
            
            st.plotly_chart(fig_density, use_container_width=True)
            
            # Analyse de segment individuel
            st.subheader("Analyse d√©taill√©e par segment")
            
            # Liste des segments pour s√©lection
            segment_options = [f"Segment {s['segment_id']}" if s['segment_id'] != "direct" else "Segment direct" 
                              for s in segment_stats if s["points_on_segment"] > 0]
            
            if segment_options:
                selected_segment = st.selectbox("S√©lectionner un segment √† analyser", segment_options)
                
                # Trouver l'index du segment s√©lectionn√©
                segment_idx = int(selected_segment.split()[-1]) if "direct" not in selected_segment else "direct"
                
                # Trouver les donn√©es du segment s√©lectionn√©
                selected_segment_data = next((s for s in segment_stats if str(s["segment_id"]) == str(segment_idx)), None)
                
                if selected_segment_data and "points_data" in selected_segment_data and selected_segment_data["points_data"]:
                    # Convertir les points du segment en DataFrame
                    points_df = pd.DataFrame([
                        {
                            "id": p.id if hasattr(p, 'id') else idx,
                            "latitude": p.latitude if hasattr(p, 'latitude') else p['latitude'],
                            "longitude": p.longitude if hasattr(p, 'longitude') else p['longitude'],
                            "speed": p.speed if hasattr(p, 'speed') else p['speed']
                        }
                        for idx, p in enumerate(selected_segment_data["points_data"])
                    ])
                    
                    # Graphique d'√©volution de la vitesse
                    fig_speed_evolution = px.line(
                        points_df, 
                        x=range(len(points_df)), 
                        y="speed",
                        markers=True,
                        labels={"speed": "Vitesse (m/s)", "x": "Points"},
                        title=f"√âvolution de la vitesse sur {selected_segment}"
                    )
                    
                    st.plotly_chart(fig_speed_evolution, use_container_width=True)
                    
                    # Statistiques du segment
                    st.subheader(f"Statistiques d√©taill√©es de {selected_segment}")
                    
                    stat_col1, stat_col2, stat_col3, stat_col4 = st.columns(4)
                    
                    with stat_col1:
                        st.metric("Nombre de points", len(points_df))
                    
                    with stat_col2:
                        st.metric("Distance", f"{selected_segment_data['distance_km']:.2f} km")
                    
                    with stat_col3:
                        st.metric("Vitesse moyenne", f"{selected_segment_data['avg_speed']:.2f} m/s")
                    
                    with stat_col4:
                        if "direct" not in selected_segment:
                            # Calculer l'efficacit√© du segment par rapport au direct
                            direct_segment = next((s for s in segment_stats if s["segment_id"] == "direct"), None)
                            if direct_segment:
                                efficacite = (direct_segment["distance_km"] / total_distance) * 100
                                st.metric("Efficacit√©", f"{efficacite:.1f}%", 
                                          delta=f"{100-efficacite:.1f}%" if efficacite < 100 else None,
                                          delta_color="inverse")
                else:
                    st.info(f"Aucune donn√©e disponible pour {selected_segment}")
            else:
                st.info("Aucun segment avec des donn√©es disponibles pour analyse d√©taill√©e")
        else:
            st.info("Aucun segment ne contient de donn√©es de points pour afficher des graphiques comparatifs")
elif page == "Analyse de vitesse":
    st.header("Analyse d√©taill√©e de la vitesse")
    
    if len(df) > 0:
        # Convertir timestamp en datetime si ce n'est pas d√©j√† fait
        if 'timestamp' in df.columns and df['timestamp'].dtype == 'object':
            df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        # Graphique d'√©volution de la vitesse
        st.subheader("√âvolution de la vitesse")
        fig = px.line(
            df, 
            x=range(len(df)) if 'timestamp' not in df.columns else 'timestamp', 
            y='speed',
            labels={"speed": "Vitesse (m/s)", "x": "Points", "timestamp": "Temps"},
            title="√âvolution de la vitesse au fil du temps"
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Distribution des vitesses
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Distribution des vitesses")
            fig_hist = px.histogram(
                df, 
                x='speed',
                nbins=20,
                labels={"speed": "Vitesse (m/s)", "count": "Nombre de points"},
                title="Distribution des vitesses"
            )
            st.plotly_chart(fig_hist, use_container_width=True)
        
        with col2:
            st.subheader("Boxplot des vitesses")
            fig_box = px.box(
                df, 
                y='speed',
                labels={"speed": "Vitesse (m/s)"},
                title="Boxplot des vitesses"
            )
            st.plotly_chart(fig_box, use_container_width=True)
        
        # Statistiques d√©taill√©es
        st.subheader("Statistiques de vitesse")
        
        stat_col1, stat_col2, stat_col3, stat_col4 = st.columns(4)
        
        with stat_col1:
            st.metric("Vitesse moyenne", f"{df['speed'].mean():.2f} m/s")
        with stat_col2:
            st.metric("Vitesse maximale", f"{df['speed'].max():.2f} m/s")
        
        with stat_col3:
            st.metric("Vitesse minimale", f"{df['speed'].min():.2f} m/s")
        
        with stat_col4:
            st.metric("√âcart type", f"{df['speed'].std():.2f} m/s")
        
        # Carte de chaleur pour vitesse vs. position
        st.subheader("Carte de chaleur des vitesses")
        
        # Cr√©er une carte folium pour la heatmap
        m_heat = folium.Map(location=[df['latitude'].mean(), df['longitude'].mean()], zoom_start=14)
        
        # Pr√©parer les donn√©es pour la heatmap (lat, lon, speed comme poids)
        heat_data = [[row['latitude'], row['longitude'], row['speed']] for _, row in df.iterrows()]
        
        # Ajouter la heatmap
        HeatMap(
            data=heat_data,
            radius=15,
            gradient={"0.2": "blue", "0.4": "lime", "0.6": "yellow", "1.0": "red"},
            min_opacity=0.5,
            max_zoom=18
        ).add_to(m_heat)
        
        # Afficher la carte
        folium_static(m_heat, width=800, height=400)
    
    else:
        st.info("Aucune donn√©e disponible pour l'analyse de vitesse.")

elif page == "Clusters":
    st.header("Analyse des clusters")
    
    if len(df) < 10:
        st.warning("Pas assez de donn√©es pour l'analyse de clusters. Minimum 10 points requis.")
    else:
        # V√©rifier si les mod√®les ML sont disponibles
        if not st.session_state.ml_models['is_trained']:
            st.warning("Mod√®les ML non entra√Æn√©s. Veuillez les entra√Æner d'abord.")
            if st.button("Entra√Æner les mod√®les ML"):
                with st.spinner("Entra√Ænement des mod√®les en cours..."):
                    success = train_ml_models()
                    if success:
                        st.success("Mod√®les ML entra√Æn√©s avec succ√®s!")
                        st.rerun()
                    else:
                        st.error("√âchec de l'entra√Ænement des mod√®les ML")
        else:
            # Param√®tres du clustering
            st.subheader("Param√®tres de clustering")
            
            param_col1, param_col2 = st.columns(2)
            
            with param_col1:
                eps = st.slider("Distance maximale entre points (m√®tres)", 10, 200, 50)
            
            with param_col2:
                min_samples = st.slider("Nombre minimum de points par cluster", 3, 20, 5)
            
            # Bouton pour lancer l'analyse
            if st.button("Analyser les clusters"):
                with st.spinner("Analyse des clusters en cours..."):
                    # Pr√©paration des coordonn√©es
                    coords = df[['latitude', 'longitude']].values
                    
                    # Conversion des degr√©s en radians
                    coords_rad = np.radians(coords)
                    
                    # Calcul du clustering
                    kms_per_radian = 6371.0088  # Rayon de la Terre en km
                    epsilon = eps / 1000 / kms_per_radian  # Conversion en radians
                    
                    db = DBSCAN(
                        eps=epsilon, 
                        min_samples=min_samples, 
                        algorithm='ball_tree',
                        metric='haversine'
                    ).fit(coords_rad)
                    
                    # Ajout des labels au DataFrame
                    df_clusters = df.copy()
                    df_clusters['cluster'] = db.labels_
                    
                    # Cr√©ation de la carte avec clusters
                    m_clusters = folium.Map(location=[df['latitude'].mean(), df['longitude'].mean()], zoom_start=14)
                    
                    # Couleurs pour les clusters
                    colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 
                            'darkblue', 'darkgreen', 'cadetblue', 'darkpurple']
                    
                    # Cr√©er un dictionnaire pour stocker les points de chaque cluster
                    cluster_points = {}
                    
                    for idx, row in df_clusters.iterrows():
                        cluster_id = int(row['cluster'])
                        
                        # Ajouter le point au dictionnaire pour traitement ult√©rieur
                        if cluster_id != -1:  # Ignorer les points de bruit pour les polygones
                            if cluster_id not in cluster_points:
                                cluster_points[cluster_id] = []
                            cluster_points[cluster_id].append([row['latitude'], row['longitude']])
                        
                        # Afficher les points sur la carte
                        if cluster_id == -1:
                            # Points de bruit
                            folium.CircleMarker(
                                [row['latitude'], row['longitude']],
                                radius=3,  # Plus petit pour les points de bruit
                                color='grey',
                                fill=True,
                                fill_opacity=0.5,
                                popup=f"Bruit: {row['speed']:.2f} m/s"
                            ).add_to(m_clusters)
                        else:
                            # Points de cluster
                            color_idx = cluster_id % len(colors)
                            folium.CircleMarker(
                                [row['latitude'], row['longitude']],
                                radius=5,
                                color=colors[color_idx],
                                fill=True,
                                fill_opacity=0.7,
                                popup=f"Cluster {cluster_id}: {row['speed']:.2f} m/s"
                            ).add_to(m_clusters)
                    
                    # Dessiner les polygones de cluster avec une couleur transparente
                    for cluster_id, points in cluster_points.items():
                        if len(points) >= 3:  # Besoin d'au moins 3 points pour un polygone
                            # Calculer l'enveloppe convexe
                            try:
                                points_array = np.array(points)
                                hull = ConvexHull(points_array)
                                
                                # Extraire les points de l'enveloppe
                                hull_points = [points_array[hull.vertices, :]]
                                
                                # Couleur du cluster
                                color_idx = cluster_id % len(colors)
                                
                                # Dessiner le polygone
                                folium.Polygon(
                                    locations=hull_points,
                                    color=colors[color_idx],
                                    fill=True,
                                    fill_color=colors[color_idx],
                                    fill_opacity=0.2,
                                    weight=2,
                                    popup=f"Zone Cluster {cluster_id}"
                                ).add_to(m_clusters)
                            except:
                                # Ignorer si le calcul de l'enveloppe √©choue
                                pass
                    
                    # Ajouter les centres des clusters avec taille proportionnelle au nombre de points
                    for cluster_id, points in cluster_points.items():
                        # Nombre de points dans le cluster
                        num_points = len(points)
                        
                        # Calculer le centre du cluster
                        center_lat = sum(p[0] for p in points) / num_points
                        center_lon = sum(p[1] for p in points) / num_points
                        
                        # Couleur du cluster
                        color_idx = cluster_id % len(colors)
                        
                        # Calculer la taille de l'ic√¥ne en fonction du nombre de points
                        # Limiter la taille pour les grands clusters
                        icon_size = min(30, 10 + (num_points / 10))
                        
                        # Ajouter un marqueur pour le centre
                        folium.CircleMarker(
                            [center_lat, center_lon],
                            radius=icon_size,
                            color=colors[color_idx],
                            fill=True,
                            fill_opacity=0.8,
                            weight=3,
                            popup=f"Centre Cluster {cluster_id}<br>Nombre de points: {num_points}"
                        ).add_to(m_clusters)
                        
                        # Ajouter un texte pour identifier le cluster
                        folium.map.Marker(
                            [center_lat, center_lon],
                            icon=folium.DivIcon(
                                icon_size=(20, 20),
                                icon_anchor=(10, 10),
                                html=f'<div style="font-size: 10pt; color: white; font-weight: bold; text-align: center;">{cluster_id}</div>'
                            )
                        ).add_to(m_clusters)
                    
                    # Afficher la carte avec clusters
                    st.subheader("Carte des clusters identifi√©s")
                    folium_static(m_clusters, width=800, height=500)
                    
                    # Statistiques des clusters
                    st.subheader("Statistiques des clusters")
                    
                    # Nombre de clusters
                    n_clusters = len(set(db.labels_)) - (1 if -1 in db.labels_ else 0)
                    n_noise = list(db.labels_).count(-1)
                    
                    col1, col2 = st.columns(2)
                    col1.metric("Nombre de clusters", n_clusters)
                    col2.metric("Points de bruit", n_noise)
# Distribution des points par cluster
                    if n_clusters > 0:
                        cluster_counts = df_clusters[df_clusters['cluster'] != -1]['cluster'].value_counts().sort_index()
                        fig = px.bar(
                            x=cluster_counts.index, 
                            y=cluster_counts.values,
                            labels={"x": "Cluster ID", "y": "Nombre de points"},
                            title="Distribution des points par cluster"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                    
                    # Vitesse moyenne par cluster
                    if n_clusters > 0:
                        cluster_speeds = df_clusters[df_clusters['cluster'] != -1].groupby('cluster')['speed'].mean()
                        fig = px.bar(
                            x=cluster_speeds.index, 
                            y=cluster_speeds.values,
                            labels={"x": "Cluster ID", "y": "Vitesse moyenne (m/s)"},
                            title="Vitesse moyenne par cluster"
                        )
                        st.plotly_chart(fig, use_container_width=True)
                        
                    # Tableau r√©capitulatif des clusters
                    if n_clusters > 0:
                        st.subheader("R√©capitulatif des clusters")
                        
                        # Cr√©er un dataframe r√©capitulatif
                        cluster_summary = []
                        
                        for cluster_id in range(n_clusters):
                            cluster_data = df_clusters[df_clusters['cluster'] == cluster_id]
                            
                            if len(cluster_data) > 0:
                                summary = {
                                    "Cluster ID": cluster_id,
                                    "Nombre de points": len(cluster_data),
                                    "Vitesse moyenne (m/s)": cluster_data['speed'].mean(),
                                    "Vitesse min (m/s)": cluster_data['speed'].min(),
                                    "Vitesse max (m/s)": cluster_data['speed'].max(),
                                    "Centre latitude": cluster_data['latitude'].mean(),
                                    "Centre longitude": cluster_data['longitude'].mean()
                                }
                                
                                if 'activity' in cluster_data.columns:
                                    top_activity = cluster_data['activity'].value_counts().idxmax()
                                    summary["Activit√© principale"] = top_activity
                                
                                cluster_summary.append(summary)
                        
                        summary_df = pd.DataFrame(cluster_summary)
                        st.dataframe(summary_df)

elif page == "Anomalies":
    st.header("D√©tection d'anomalies")
    
    if len(df) < 10:
        st.warning("Pas assez de donn√©es pour la d√©tection d'anomalies. Minimum 10 points requis.")
    else:
        # V√©rifier si les mod√®les ML sont disponibles
        if not st.session_state.ml_models['is_trained']:
            st.warning("Mod√®les ML non entra√Æn√©s. Veuillez les entra√Æner d'abord.")
            if st.button("Entra√Æner les mod√®les ML"):
                with st.spinner("Entra√Ænement des mod√®les en cours..."):
                    success = train_ml_models()
                    if success:
                        st.success("Mod√®les ML entra√Æn√©s avec succ√®s!")
                        st.rerun()
                    else:
                        st.error("√âchec de l'entra√Ænement des mod√®les ML")
        else:
            # Options pour la d√©tection d'anomalies
            st.subheader("Param√®tres de d√©tection")
            
            anomaly_method = st.radio(
                "M√©thode de d√©tection d'anomalies",
                ["Bas√©e sur un mod√®le", "Bas√©e sur des r√®gles"]
            )
            
            if anomaly_method == "Bas√©e sur un mod√®le":
                # Option avec mod√®le d'apprentissage
                contamination = st.slider("Pourcentage d'anomalies attendu", 0.01, 0.2, 0.05, 0.01)
                features = st.multiselect(
                    "Caract√©ristiques √† utiliser pour la d√©tection",
                    ["speed", "latitude", "longitude"],
                    default=["speed"]
                )
                
                # Bouton pour ex√©cuter la d√©tection d'anomalies
                if st.button("D√©tecter les anomalies (mod√®le)"):
                    with st.spinner("D√©tection des anomalies en cours..."):
                        if not features:
                            st.error("Veuillez s√©lectionner au moins une caract√©ristique.")
                        else:
                            # Pr√©paration des donn√©es
                            X = df[features].values
                            
                            # Entra√Ænement du mod√®le Isolation Forest
                            model = IsolationForest(
                                contamination=contamination, 
                                random_state=42, 
                                n_estimators=100
                            )
                            
                            # Pr√©diction des anomalies
                            pred = model.fit_predict(X)
                            scores = model.decision_function(X)
                            
                            # Convertir les pr√©dictions
                            # IsolationForest retourne 1 pour les points normaux et -1 pour les anomalies
                            df_anomalies = df.copy()
                            df_anomalies['anomaly'] = pred
                            df_anomalies['anomaly_score'] = scores
                            
                            # Afficher les r√©sultats de la d√©tection
                            display_anomaly_results(df_anomalies)
            
            else:  # M√©thode bas√©e sur des r√®gles
                st.info("Cette m√©thode identifie les anomalies bas√©es sur des r√®gles simples d√©finies par l'utilisateur.")
                
                # D√©finir les r√®gles pour les anomalies
                speed_threshold_low = st.slider("Vitesse anormalement basse (m/s)", 0.0, 5.0, 0.1, 0.1)
                speed_threshold_high = st.slider("Vitesse anormalement √©lev√©e (m/s)", 5.0, 30.0, 15.0, 0.5)
                
                # Bouton pour ex√©cuter la d√©tection d'anomalies
                if st.button("D√©tecter les anomalies (r√®gles)"):
                    with st.spinner("D√©tection des anomalies en cours..."):
                        # Cr√©er une copie du DataFrame
                        df_anomalies = df.copy()
                        
                        # D√©finir les anomalies selon les r√®gles
                        df_anomalies['anomaly'] = 1  # Par d√©faut, tous les points sont normaux
                        
                        # Marquer les anomalies (vitesse trop basse ou trop √©lev√©e)
                        df_anomalies.loc[df_anomalies['speed'] <= speed_threshold_low, 'anomaly'] = -1
                        df_anomalies.loc[df_anomalies['speed'] >= speed_threshold_high, 'anomaly'] = -1
                        
                        # Calcul d'un score d'anomalie simplifi√©
                        # Distance normalis√©e par rapport aux seuils
                        def calculate_score(speed):
                            if speed <= speed_threshold_low:
                                return -1 * (speed_threshold_low - speed) / speed_threshold_low
                            elif speed >= speed_threshold_high:
                                return -1 * (speed - speed_threshold_high) / speed_threshold_high
                            else:
                                # Distance relative au seuil le plus proche
                                dist_to_low = (speed - speed_threshold_low) / (speed_threshold_high - speed_threshold_low)
                                dist_to_high = (speed_threshold_high - speed) / (speed_threshold_high - speed_threshold_low)
                                return min(dist_to_low, dist_to_high)
                        
                        df_anomalies['anomaly_score'] = df_anomalies['speed'].apply(calculate_score)
                        
                        # Afficher les r√©sultats de la d√©tection
                        display_anomaly_results(df_anomalies)
            

elif page == "Statistiques":
    st.header("Statistiques g√©n√©rales")
    
    if len(df) == 0:
        st.info("Aucune donn√©e disponible pour les statistiques.")
    else:
        # M√©triques de base
        col1, col2, col3 = st.columns(3)
        col1.metric("Total des points", len(df))
        col2.metric("Distance estim√©e", f"{calculate_distance(df):.2f} km")
        if 'timestamp' in df.columns:
            col3.metric("Dur√©e", calculate_duration(df))
        
        # Graphiques suppl√©mentaires
        col1, col2 = st.columns(2)
        
        with col1:
            # Nuage de points 2D (scatter plot)
            fig = px.scatter(df, x='longitude', y='latitude', color='speed',
                          color_continuous_scale='Viridis',
                          labels={"longitude": "Longitude", "latitude": "Latitude", "speed": "Vitesse (m/s)"},
                          title="Nuage de points g√©ographiques")
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Heatmap 2D des positions
            fig = px.density_heatmap(df, x='longitude', y='latitude',
                                   labels={"longitude": "Longitude", "latitude": "Latitude"},
                                   title="Distribution spatiale des points (heatmap)")
            st.plotly_chart(fig, use_container_width=True)
        
        if 'activity' in df.columns:
            st.subheader("Distribution par activit√©")
            fig = px.pie(df, names='activity', title="R√©partition des activit√©s")
            st.plotly_chart(fig, use_container_width=True)
        
        # Statistiques avanc√©es
        st.subheader("Statistiques avanc√©es")
        
        col1, col2 = st.columns(2)
        
        # Cr√©er des cat√©gories de vitesse pour l'analyse
        speed_categories = {
            '0-1 m/s': 0,
            '1-2 m/s': 0,
            '2-5 m/s': 0,
            '5-10 m/s': 0,
            '10+ m/s': 0
        }
        
        # Compter les points dans chaque cat√©gorie
        for speed in df['speed']:
            if speed < 1:
                speed_categories['0-1 m/s'] += 1
            elif speed < 2:
                speed_categories['1-2 m/s'] += 1
            elif speed < 5:
                speed_categories['2-5 m/s'] += 1
            elif speed < 10:
                speed_categories['5-10 m/s'] += 1
            else:
                speed_categories['10+ m/s'] += 1
        
        with col1:
            # Histogramme des vitesses par cat√©gorie
            fig = px.bar(
                x=list(speed_categories.keys()),
                y=list(speed_categories.values()),
                labels={"x": "Cat√©gorie de vitesse", "y": "Nombre de points"},
                title="Distribution des vitesses par cat√©gorie"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            if 'timestamp' in df.columns and df['timestamp'].dtype != 'object':
                # Extraire l'heure
                df_hour = df.copy()
                df_hour['hour'] = df_hour['timestamp'].dt.hour
                
                # Histogramme par heure
                fig = px.histogram(
                    df_hour,
                    x='hour',
                    labels={"hour": "Heure de la journ√©e", "count": "Nombre de points"},
                    title="Distribution des points par heure"
                )
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("Donn√©es temporelles non disponibles pour l'analyse horaire.")
        
        # Visualisation 3D de la densit√© des points
        st.subheader("Visualisation 3D de la densit√©")
        
        # Cr√©er un histogramme 2D pour la visualisation 3D
        hist, x_edges, y_edges = np.histogram2d(
            df['longitude'], 
            df['latitude'], 
            bins=[20, 20]
        )
        
        # Coordonn√©es X et Y pour le graphique 3D
        x_centers = (x_edges[:-1] + x_edges[1:]) / 2
        y_centers = (y_edges[:-1] + y_edges[1:]) / 2
        x_mesh, y_mesh = np.meshgrid(x_centers, y_centers)
        
        # Cr√©er la visualisation 3D avec Plotly
        fig = go.Figure(data=[go.Surface(
            z=hist.T,
            x=x_mesh,
            y=y_mesh,
            colorscale='Viridis',
            opacity=0.8
        )])
        
        fig.update_layout(
            title="Distribution 3D des points",
            scene=dict(
                xaxis_title='Longitude',
                yaxis_title='Latitude',
                zaxis_title='Nombre de points'
            ),
            width=800,
            height=600,
            margin=dict(l=65, r=50, b=65, t=90)
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # Afficher les donn√©es brutes
        if st.checkbox("Afficher les donn√©es brutes"):
            st.subheader("Donn√©es brutes")
            st.dataframe(df)

elif page == "Configuration":
    st.header("Configuration du syst√®me")
    
    # Onglets pour diff√©rentes options de configuration
    config_tabs = st.tabs(["Ajout manuel", "Import CSV", "Export", "Mod√®les ML"])
    
    # Onglet Ajout manuel
    with config_tabs[0]:
        st.subheader("Ajouter manuellement un point")
        
        # Formulaire pour ajouter un point
        with st.form("add_point_form"):
            point_col1, point_col2, point_col3 = st.columns(3)
            
            with point_col1:
                latitude = st.number_input("Latitude", value=49.2484420, format="%.7f")
            
            with point_col2:
                longitude = st.number_input("Longitude", value=4.0415017, format="%.7f")
            
            with point_col3:
                speed = st.number_input("Vitesse (m/s)", value=1.0, min_value=0.0, format="%.2f")
            
            submit_button = st.form_submit_button("Ajouter ce point")
            
        if submit_button:
            # Ajouter le point √† la base de donn√©es
            result = push_data(latitude, longitude, speed)
            
            if result["status"] == "success":
                st.success(f"Point ajout√© avec succ√®s! Vitesse: {speed} m/s")
                
                # Afficher la carte avec le nouveau point
                m = folium.Map(location=[latitude, longitude], zoom_start=16)
                
                # Ajouter le marqueur
                folium.Marker(
                    [latitude, longitude],
                    popup=f"Nouveau point: {speed} m/s",
                    icon=folium.Icon(color="green")
                ).add_to(m)
                
                folium_static(m, width=700, height=300)
                
                # Proposer d'actualiser les donn√©es
                if st.button("Actualiser les donn√©es"):
                    st.rerun()
            else:
                st.error(f"Erreur lors de l'ajout du point: {result.get('message', 'Erreur inconnue')}")
    
    # Onglet Import CSV
    with config_tabs[1]:
        st.subheader("Importer des donn√©es CSV")
        
        # Uploader un fichier CSV
        uploaded_file = st.file_uploader("Choisir un fichier CSV", type="csv")
        
        if uploaded_file is not None:
            # Analyser les donn√©es
            try:
                csv_data = pd.read_csv(uploaded_file)
                
                # V√©rifier si les colonnes requises sont pr√©sentes
                required_cols = ['latitude', 'longitude', 'speed']
                missing_cols = [col for col in required_cols if col not in csv_data.columns]
                
                if missing_cols:
                    st.error(f"Colonnes manquantes dans le CSV: {', '.join(missing_cols)}")
                    st.info("Le CSV doit contenir au minimum les colonnes: latitude, longitude et speed")
                else:
                    # Afficher un aper√ßu
                    st.subheader("Aper√ßu des donn√©es")
                    st.dataframe(csv_data.head())
                    
                    # Afficher les statistiques
                    st.subheader("Statistiques")
                    st.write(f"Nombre de points: {len(csv_data)}")
                    st.write(f"Vitesse moyenne: {csv_data['speed'].mean():.2f} m/s")
                    
                    # Option pour importer
                    if st.button("Importer ces donn√©es"):
                        with st.spinner("Importation en cours..."):
                            # Cr√©er une session de base de donn√©es
                            session = Session()
                            
                            # Compter les lignes import√©es
                            success_count = 0
                            errors = []
                            
                            for _, row in csv_data.iterrows():
                                try:
                                    # Extraire les donn√©es
                                    lat = float(row['latitude'])
                                    lon = float(row['longitude'])
                                    spd = float(row['speed'])
                                    
                                    # Cr√©er un nouvel enregistrement
                                    new_data = SensorData(
                                        latitude=lat,
                                        longitude=lon,
                                        speed=spd
                                    )
                                    
                                    # Ajouter √† la session
                                    session.add(new_data)
                                    success_count += 1
                                except Exception as e:
                                    errors.append(str(e))
                            
                            # Valider la transaction
                            if success_count > 0:
                                session.commit()
                            
                            session.close()
                            
                            # Afficher les r√©sultats
                            if success_count > 0:
                                st.success(f"{success_count} points import√©s avec succ√®s!")
                                
                                if errors:
                                    st.warning(f"{len(errors)} erreurs rencontr√©es.")
                                    if st.checkbox("Afficher les erreurs"):
                                        for i, err in enumerate(errors[:10]):  # Limiter √† 10 erreurs
                                            st.text(f"Erreur {i+1}: {err}")
                                
                                # Proposer d'actualiser les donn√©es
                                if st.button("Actualiser les donn√©es"):
                                    st.rerun()
                            else:
                                st.error("Aucun point n'a pu √™tre import√©.")
                                if errors:
                                    st.error(f"Erreurs: {errors[0]}")
                    
            except Exception as e:
                st.error(f"Erreur lors de l'analyse du CSV: {str(e)}")
    
    # Onglet Export
    with config_tabs[2]:
        st.subheader("Exporter les donn√©es")
        
        export_format = st.selectbox("Format d'export", ["CSV", "JSON"])
        
        if st.button("Exporter les donn√©es"):
            # R√©cup√©rer toutes les donn√©es
            data = get_all_points()
            
            if data["status"] == "success" and data["points"]:
                points_df = pd.DataFrame(data["points"])
                
                # Cr√©er le fichier d'export
                if export_format == "CSV":
                    csv_export = points_df.to_csv(index=False)
                    st.download_button(
                        label="T√©l√©charger le CSV",
                        data=csv_export,
                        file_name=f"sensor_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                else:  # JSON
                    json_export = points_df.to_json(orient="records")
                    st.download_button(
                        label="T√©l√©charger le JSON",
                        data=json_export,
                        file_name=f"sensor_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json"
                    )
                
                st.success(f"{len(points_df)} points export√©s avec succ√®s!")
            else:
                st.error("Aucune donn√©e disponible pour l'export.")
    
    # Onglet Mod√®les ML
    with config_tabs[3]:
        st.subheader("Gestion des mod√®les ML")
        
        # V√©rifier l'√©tat des mod√®les
        is_trained = st.session_state.ml_models['is_trained']
        
        status_col1, status_col2 = st.columns(2)
        
        with status_col1:
            status_text = "‚úÖ Entra√Æn√©s" if is_trained else "‚ùå Non entra√Æn√©s"
            st.info(f"Statut des mod√®les ML: {status_text}")
        
        with status_col2:
            session = Session()
            point_count = session.query(SensorData).count()
            session.close()
            
            st.info(f"Nombre de points disponibles: {point_count}")
            if point_count < 10:
                st.warning("Minimum 10 points requis pour l'entra√Ænement")
        
        # Options pour g√©rer les mod√®les
        if is_trained:
            if st.button("R√©entra√Æner les mod√®les"):
                with st.spinner("R√©entra√Ænement en cours..."):
                    success = train_ml_models()
                    if success:
                        st.success("Mod√®les ML r√©entra√Æn√©s avec succ√®s!")
                    else:
                        st.error("√âchec du r√©entra√Ænement des mod√®les ML")
            
            # Option pour exporter les mod√®les
            if st.button("Exporter les mod√®les"):
                try:
                    model_dir = "ml_models"
                    os.makedirs(model_dir, exist_ok=True)
                    
                    # Sauvegarder les mod√®les
                    if st.session_state.ml_models['isolation_forest'] is not None:
                        joblib.dump(
                            st.session_state.ml_models['isolation_forest'], 
                            os.path.join(model_dir, "iso_forest.pkl")
                        )
                    
                    if st.session_state.ml_models['dbscan'] is not None:
                        joblib.dump(
                            st.session_state.ml_models['dbscan'], 
                            os.path.join(model_dir, "dbscan.pkl")
                        )
                    
                    st.success("Mod√®les ML export√©s avec succ√®s!")
                except Exception as e:
                    st.error(f"Erreur lors de l'exportation des mod√®les: {str(e)}")
        else:
            if st.button("Entra√Æner les mod√®les"):
                with st.spinner("Entra√Ænement en cours..."):
                    if point_count >= 10:
                        success = train_ml_models()
                        if success:
                            st.success("Mod√®les ML entra√Æn√©s avec succ√®s!")
                            # Recommander de rafra√Æchir la page
                            st.info("Veuillez actualiser la page pour utiliser les mod√®les")
                            if st.button("Actualiser la page"):
                                st.rerun()
                        else:
                            st.error("√âchec de l'entra√Ænement des mod√®les ML")
                    else:
                        st.error("Pas assez de donn√©es pour l'entra√Ænement (minimum 10 points)")

# D√©marrer le serveur Flask √† la fin de votre fichier
if __name__ == "__main__":
    # D√©marrer Flask dans un thread s√©par√©
    flask_thread = threading.Thread(target=run_flask_server)
    flask_thread.daemon = True  # Le thread se terminera quand Streamlit se termine
    flask_thread.start()
    
    # Informer l'utilisateur que l'API est d√©marr√©e
    print(f"üöÄ API Flask d√©marr√©e sur le port 5001 pour recevoir les donn√©es de l'app Swift")
    
    # Le reste du code de d√©marrage de Streamlit
    if not st.session_state.ml_models['is_trained']:
        load_ml_models()